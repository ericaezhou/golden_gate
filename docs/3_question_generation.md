# Question Generation & Reconciliation (Step 3)

The question pipeline collects knowledge gaps from two independent sources — per-file deep dives and cross-file global analysis — then merges, deduplicates, and prioritizes them into a unified backlog ready for the [interview loop](4_interview_loop.md).

## Pipeline Position

```
File Upload → Parsing → Deep Dives → Question Generation (this doc) → Interview → Summary
```

This doc covers three nodes that together produce the unified question set:

| Node | What it does |
|------|--------------|
| `concatenate_deep_dives` | Extracts per-file questions (Q1) from deep dive reports |
| `global_summarize` | Generates cross-file questions (Q2) from global analysis |
| `reconcile_questions` | Merges Q1+Q2, deduplicates, auto-resolves, caps at 15 |

## Architecture

```
deep_dive_reports (per file)
    ↓
concatenate_deep_dives
    ├── deep_dive_corpus (text)
    └── question_backlog (Q1: per-file questions)
            ↓
global_summarize
    ├── global_summary (text)
    └── question_backlog (Q1 + Q2: per-file + global questions)
            ↓
reconcile_questions (LLM-assisted)
    ├── KEEP — unique, unanswered → assign P0/P1/P2
    ├── MERGE — duplicate of another → mark as merged
    └── ANSWER — already answered by corpus → auto-resolve
            ↓
question_backlog (unified, capped at 15 open) → Interview
```

## Question Sources

### Q1: Per-File Questions (origin=`PER_FILE`)

Generated during each deep dive pass. Each pass can produce up to `MAX_QUESTIONS_PER_FILE` (default: 5) questions about that specific file.

**What they capture:**
- Why specific values, thresholds, or constants were chosen
- How manual steps or overrides work
- Edge cases or failure modes in the file's logic
- Undocumented assumptions embedded in formulas or code

**Source node:** `concatenate_deep_dives` (extracts from `DeepDiveReport.questions`)

**Question ID format:** `{file_id}-q{n}` (e.g., `model_xlsx_a1b2c3-q1`)

### Q2: Global Questions (origin=`GLOBAL`)

Generated by `global_summarize` after analyzing the concatenated corpus across all files. These questions can ONLY be discovered by reasoning across multiple files simultaneously.

**What they capture:**
- Assumption mismatches — one file says X, another uses Y
- Workflow dependencies — which file's output feeds another
- Missing context that spans multiple artifacts
- Undocumented decision criteria affecting multiple files

**Source node:** `global_summarize`

**Question ID format:** `global-{uuid8}` (e.g., `global-a1b2c3d4`)

Each Q2 question includes:
- `involved_files` — which files are relevant
- `evidence` — the specific mismatch or gap that prompted the question
- `priority` — P0/P1/P2

## Question Data Model

Defined in `backend/models/questions.py`:

```python
class Question(BaseModel):
    question_id: str
    question_text: str
    origin: QuestionOrigin       # PER_FILE | GLOBAL | FOLLOW_UP
    source_file_id: str | None   # Which file generated it (Q1 only)
    evidence: list[Evidence]     # Supporting evidence from files
    priority: QuestionPriority   # P0 | P1 | P2
    status: QuestionStatus       # OPEN | ANSWERED_BY_FILES | ANSWERED_BY_INTERVIEW
                                 # | MERGED | DEPRIORITIZED
    answer: str | None
    confidence: float | None     # 0.0 – 1.0
```

### Priority Levels

| Priority | Meaning | Example |
|----------|---------|---------|
| P0 | Total knowledge-loss risk — undocumented, lives only in person's head | "Why is the discount rate set to 12.7%?" |
| P1 | Partial/ambiguous — some documentation but incomplete | "The model references 'adjusted_revenue' but it's calculated differently in two sheets" |
| P2 | Nice-to-have clarification | "Is this quarterly rollup still used?" |

### Status Lifecycle

```
OPEN → ANSWERED_BY_FILES    (auto-resolved during reconciliation)
OPEN → ANSWERED_BY_INTERVIEW (answered during interview)
OPEN → MERGED               (duplicate of another question)
OPEN → DEPRIORITIZED        (dropped to stay under cap)
```

## Concatenate Deep Dives

**Source file:** `backend/nodes/concatenate.py`

After all parallel deep dives complete, this node:

1. Groups reports by file, keeps only the latest (highest) pass per file
2. Formats each report as a readable markdown section
3. Joins all sections into `deep_dive_corpus` (a single string)
4. Extracts each report's `questions` list into `Question` objects with `origin=PER_FILE`

**State writes:**
- `deep_dive_corpus` — concatenated text of all file analyses
- `question_backlog` — initialized with Q1 questions (all status=`OPEN`)

**Persistence:** `deep_dive_corpus.json`

## Global Summarize

**Source file:** `backend/nodes/global_summarize.py`

Sends the full `deep_dive_corpus` to the LLM for cross-file reasoning. Produces a project-wide narrative and generates NEW questions that only emerge from looking across files.

**LLM call:** Single `call_llm_json()` returning:
```json
{
  "global_summary": "<project-wide narrative>",
  "questions": [
    {
      "text": "<the question>",
      "priority": "P0",
      "involved_files": ["file_id_1", "file_id_2"],
      "evidence": "<what motivated this question>"
    }
  ]
}
```

**Key behavior:**
- Q2 questions have `origin=QuestionOrigin.GLOBAL`
- Does NOT merge or deduplicate — just APPENDS Q2 to existing Q1 backlog
- Deduplication happens downstream in `reconcile_questions`
- `question_backlog` has **no reducer** in state, so the returned list fully replaces the previous value

**State writes:**
- `global_summary` — project-wide narrative
- `question_backlog` — replaced with `existing_Q1 + new_Q2`

**Persistence:** `global_summary.json`

## Reconcile Questions

**Source file:** `backend/nodes/reconcile_questions.py`

LLM-assisted cleanup of the combined Q1+Q2 backlog. This is the final step before the interview — it produces a focused, prioritized question set.

**LLM call:** Single `call_llm_json()` that decides, for EACH question, one action:

| Action | What happens | When |
|--------|-------------|------|
| **keep** | Question stays open, priority may be updated | Unique and unanswered |
| **merge** | Status → `MERGED` | Semantically duplicate of another question |
| **answer** | Status → `ANSWERED_BY_FILES`, answer text populated | Corpus or global summary already answers it clearly |

**Cap enforcement:** After LLM decisions, if more than `MAX_OPEN_QUESTIONS` (default: 15) remain open:
1. Sort open questions by priority (P0 first, then P1, then P2)
2. Keep the top 15
3. Mark the rest as `DEPRIORITIZED`

**State reads:**
- `question_backlog` — combined Q1+Q2 from `global_summarize`
- `deep_dive_corpus` — evidence for auto-answering
- `global_summary` — evidence for auto-answering

**State writes:**
- `question_backlog` — cleaned, reconciled list (full replacement)

**Persistence:** `question_backlog.json`

## State Flow Summary

```
concatenate_deep_dives
  reads:  deep_dive_reports
  writes: deep_dive_corpus, question_backlog (Q1 only)

global_summarize
  reads:  deep_dive_corpus, structured_files, question_backlog (Q1)
  writes: global_summary, question_backlog (Q1 + Q2)

reconcile_questions
  reads:  question_backlog (Q1 + Q2), deep_dive_corpus, global_summary
  writes: question_backlog (reconciled, capped)
```

**Important:** `question_backlog` has NO reducer in `OffboardingState` — each node returns the full replacement list. This is intentional: each step needs to see and modify the complete backlog.

## Storage

```
data/sessions/{session_id}/
├── deep_dive_corpus.json       # Concatenated per-file analysis text
├── global_summary.json         # Cross-file narrative + global questions
└── question_backlog.json       # Final reconciled question list
```

## What Happens Next

The reconciled question backlog flows into the [interview loop](4_interview_loop.md), where open P0/P1 questions are asked to the departing employee in a conversational, human-in-the-loop interview.
